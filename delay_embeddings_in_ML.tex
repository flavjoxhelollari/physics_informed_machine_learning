\documentclass{article}


\title{Delay Embeddings}
\author{flavjo.xhelollari }
\date{April 2025}

\begin{document}

\maketitle

Delay embeddings provide a powerful tool for reconstructing the phase space of a dynamical system from scalar time series data. Rooted in Takens' theorem, they allow one to recover a topologically equivalent embedding of the underlying state space dynamics by using time-delayed versions of a single observable. This is particularly relevant in physics, where systems are often described in terms of their full phase space (e.g., position and momentum), but experimental limitations typically restrict us to partial observations. By leveraging delay embeddings, one can reconstruct a high-dimensional representation of the latent state, effectively enabling the analysis and modeling of complex dynamical behavior from limited measurements.

In the realm of machine learning, especially within physics-informed frameworks, delay embeddings serve as a bridge between raw data and structured representations of physical systems. Hamiltonian Neural Networks (HNNs) and Lagrangian Neural Networks (LNNs) are two such models that incorporate physical priors into their architectures. HNNs learn a scalar Hamiltonian function \( H(q, p) \), ensuring energy-conserving dynamics by parameterizing the equations of motion through symplectic structure. LNNs, on the other hand, learn a Lagrangian \( \mathcal{L}(q, \dot{q}) \) and use the Euler–Lagrange equations to model time evolution. Delay embeddings play a crucial role in supplying these networks with approximate reconstructions of the full phase space from limited observations. This synergy enables the data-driven discovery of physical laws while preserving structural properties such as conservation laws and symmetries, making it a cornerstone technique in modern computational physics and scientific machine learning.


In addition to enabling phase space reconstruction from partial observations, delay embeddings can be systematically employed to quantify the fidelity with which Hamiltonian and Lagrangian neural networks (HNNs and LNNs) learn the underlying physical laws. By reconstructing the state-space geometry from scalar measurements, delay embeddings allow for the comparison of learned and true dynamics within a consistent embedded manifold. For HNNs, one can evaluate the conservation of the learned Hamiltonian function \( H(q, p) \) along predicted trajectories in the embedded space, providing a direct measure of energy-preserving behavior. Likewise, for LNNs, the correctness of the learned Lagrangian dynamics \( \mathcal{L}(q, \dot{q}) \) can be assessed by examining whether the resulting trajectories satisfy physical invariants or preserve known symmetries in the delay-embedded coordinates. Moreover, geometric and topological diagnostics—such as phase portrait similarity, Lyapunov exponents, fractal dimension, and recurrence plots—can be computed both on the original system and on the neural network outputs to quantitatively assess the accuracy of the learned model. In this way, delay embeddings serve not only as a tool for input representation but also as a rigorous basis for validating the physical consistency and generalization capabilities of learned dynamical models in real-world settings.


\end{document}
